{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-overview",
   "metadata": {},
   "source": [
    "# GAT — Graph Construction\n",
    "Transforms IMPECT event data into per-possession `torch_geometric.data.Data` objects for a Graph Attention Network.\n",
    "\n",
    "**Graph design:**\n",
    "- **Node** = one action (row) in a possession sequence\n",
    "- **Real edge** i → i+1: the sequential action transition\n",
    "- **Synthetic edge** i → i+2 and i → i+3: long-range synergy links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from kloppy import impect\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full event stream so we can detect possession changes correctly\n",
    "MATCH_ID       = 122838\n",
    "COMPETITION_ID = 743\n",
    "\n",
    "dataset = impect.load_open_data(match_id=MATCH_ID, competition_id=COMPETITION_ID)\n",
    "\n",
    "events_df = (\n",
    "    dataset\n",
    "    .transform(to_coordinate_system=\"secondspectrum\")\n",
    "    .to_df(engine=\"pandas\")\n",
    ")\n",
    "\n",
    "# Use the full event stream — NO_VIDEO events are handled at graph-build time\n",
    "df = events_df.copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"Total events: {len(df)}\")\n",
    "print(f\"Event types:  {sorted(df['event_type'].unique().tolist())}\")\n",
    "print(f\"Columns:      {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-timestamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_duration_to_seconds(ts) -> float:\n",
    "    \"\"\"\n",
    "    Convert a Kloppy duration value to float seconds.\n",
    "    Handles pd.Timedelta objects and string formats such as:\n",
    "      '0\\u00b5s', '4s 192999\\u00b5s', '50m 29s 618999\\u00b5s', '24s 350ms', '50m 39s 229ms'\n",
    "    \"\"\"\n",
    "    if ts is None or (isinstance(ts, float) and np.isnan(ts)):\n",
    "        return 0.0\n",
    "    if isinstance(ts, pd.Timedelta):\n",
    "        return ts.total_seconds()\n",
    "    total = 0.0\n",
    "    for part in str(ts).split():\n",
    "        if   part.endswith(\"\\u00b5s\"):  total += float(part[:-2]) / 1_000_000\n",
    "        elif part.endswith(\"ms\"):        total += float(part[:-2]) / 1_000\n",
    "        elif part.endswith(\"m\"):         total += float(part[:-1]) * 60\n",
    "        elif part.endswith(\"s\"):         total += float(part[:-1])\n",
    "    return round(total, 6)\n",
    "\n",
    "\n",
    "# Sanity tests\n",
    "assert abs(parse_duration_to_seconds(\"0\\u00b5s\")                - 0.0)        < 1e-9\n",
    "assert abs(parse_duration_to_seconds(\"4s 192999\\u00b5s\")        - 4.192999)   < 1e-6\n",
    "assert abs(parse_duration_to_seconds(\"50m 29s 618999\\u00b5s\")   - 3029.618999) < 1e-4\n",
    "assert abs(parse_duration_to_seconds(\"24s 350ms\")                - 24.350)     < 1e-6\n",
    "assert abs(parse_duration_to_seconds(\"50m 39s 229ms\")            - 3039.229)   < 1e-4\n",
    "print(\"Timestamp parser OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NaN fills ---\n",
    "# team_id: forward-fill so events like NO_VIDEO inherit surrounding team context\n",
    "df[\"team_id\"]           = df[\"team_id\"].ffill()\n",
    "# player_id: some events (e.g. NO_VIDEO) have no player\n",
    "df[\"player_id\"]         = df[\"player_id\"].fillna(\"UNKNOWN\")\n",
    "df[\"pass_type\"]         = df[\"pass_type\"].fillna(\"NONE\")\n",
    "df[\"is_under_pressure\"] = df[\"is_under_pressure\"].fillna(False).astype(bool)\n",
    "df[\"result\"]            = df[\"result\"].fillna(\"NONE\")\n",
    "df[\"success\"]           = df[\"success\"].fillna(False).astype(bool)\n",
    "# Fill start coords before using them to fill end coords\n",
    "df[\"coordinates_x\"]     = df[\"coordinates_x\"].fillna(0.0)\n",
    "df[\"coordinates_y\"]     = df[\"coordinates_y\"].fillna(0.0)\n",
    "df[\"end_coordinates_x\"] = df[\"end_coordinates_x\"].fillna(df[\"coordinates_x\"])\n",
    "df[\"end_coordinates_y\"] = df[\"end_coordinates_y\"].fillna(df[\"coordinates_y\"])\n",
    "\n",
    "# --- Parse timestamps and sort chronologically ---\n",
    "df[\"timestamp_sec\"] = df[\"timestamp\"].apply(parse_duration_to_seconds)\n",
    "df = df.sort_values([\"period_id\", \"timestamp_sec\"]).reset_index(drop=True)\n",
    "\n",
    "# --- Fit encoders on the full match (globally consistent IDs across all graphs) ---\n",
    "player_enc     = LabelEncoder().fit(df[\"player_id\"].astype(str))\n",
    "event_type_enc = LabelEncoder().fit(df[\"event_type\"].astype(str))\n",
    "pass_type_enc  = LabelEncoder().fit(df[\"pass_type\"].astype(str))\n",
    "result_enc     = LabelEncoder().fit(df[\"result\"].astype(str))\n",
    "\n",
    "df[\"player_id_enc\"]   = player_enc.transform(df[\"player_id\"].astype(str))\n",
    "df[\"event_type_enc\"]  = event_type_enc.transform(df[\"event_type\"].astype(str))\n",
    "df[\"pass_type_enc\"]   = pass_type_enc.transform(df[\"pass_type\"].astype(str))\n",
    "df[\"result_enc\"]      = result_enc.transform(df[\"result\"].astype(str))\n",
    "\n",
    "print(f\"Players:     {len(player_enc.classes_)} unique\")\n",
    "print(f\"Event types: {list(event_type_enc.classes_)}\")\n",
    "print(f\"Pass types:  {list(pass_type_enc.classes_)}\")\n",
    "print(f\"Results:     {list(result_enc.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_possessions(df: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a match DataFrame (all events, sorted chronologically) into\n",
    "    possession sequences.\n",
    "\n",
    "    A new possession begins when:\n",
    "      - team_id changes from the previous event, OR\n",
    "      - the previous event was a SHOT\n",
    "\n",
    "    Returns a list of DataFrames each with a reset 0-based index.\n",
    "    \"\"\"\n",
    "    possessions   = []\n",
    "    current_start = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.iloc[i - 1]\n",
    "        curr = df.iloc[i]\n",
    "\n",
    "        if prev[\"team_id\"] != curr[\"team_id\"] or prev[\"event_type\"] == \"SHOT\":\n",
    "            possessions.append(df.iloc[current_start:i].reset_index(drop=True))\n",
    "            current_start = i\n",
    "\n",
    "    possessions.append(df.iloc[current_start:].reset_index(drop=True))\n",
    "    return possessions\n",
    "\n",
    "\n",
    "possessions = segment_possessions(df)\n",
    "lengths     = [len(p) for p in possessions]\n",
    "\n",
    "print(f\"Possessions: {len(possessions)}\")\n",
    "print(f\"Size — min: {min(lengths)}, max: {max(lengths)}, mean: {np.mean(lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6gz4s4ptcxa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --- Score constants ---\n",
    "FIELD_HALF        = 52.5          # x-axis half-length (m)\n",
    "FIELD_LENGTH      = 105.0         # total pitch length (m)\n",
    "FIELD_WIDTH       = 68.0          # total pitch width (m)\n",
    "# Max distance any point on the field can be from a goal (corner-to-goal diagonal)\n",
    "MAX_FIELD_DIST    = (FIELD_LENGTH ** 2 + (FIELD_WIDTH / 2) ** 2) ** 0.5  # ≈ 110.4 m\n",
    "SIGMOID_SHIFT     = 7.5           # sigmoid midpoint — gives ≈ 0 at t=0, ≈ 1 at t=15 s\n",
    "ADVANCEMENT_WEIGHT = 1.0          # scales the linear position-from-center-field term\n",
    "\n",
    "# End-of-play event bonuses\n",
    "GOAL_BONUS        =  1.0\n",
    "SHOT_BONUS        =  0.3\n",
    "OUT_OF_BOUNDS_PEN = -0.1\n",
    "INTERCEPT_PEN     = -0.2\n",
    "\n",
    "\n",
    "def get_attacking_goal_x(team_id: str, period_id: int, home_team_id: str) -> float:\n",
    "    \"\"\"\n",
    "    Return the x-coordinate of the opponent's goal for the possessing team.\n",
    "\n",
    "    Convention assumed for Kloppy secondspectrum + IMPECT Bundesliga data:\n",
    "      Period 1 — home team attacks toward +x  (opponent's goal at x = +52.5)\n",
    "      Period 2 — teams switch; home attacks toward -x (opponent's goal at x = -52.5)\n",
    "    \"\"\"\n",
    "    is_home = str(team_id) == str(home_team_id)\n",
    "    attacks_positive = (is_home and period_id == 1) or (not is_home and period_id == 2)\n",
    "    return FIELD_HALF if attacks_positive else -FIELD_HALF\n",
    "\n",
    "\n",
    "def score_play(play_clean: pd.DataFrame, attacking_goal_x: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute scalar play score P for one possession sequence (NO_VIDEO removed).\n",
    "\n",
    "        P = (displacement_scaled * advancement_factor * sigmoid_t) + end_bonus\n",
    "\n",
    "    Components\n",
    "    ----------\n",
    "    displacement_scaled : float [-1, 1]\n",
    "        How much the ball moved toward the opponent's goal during the play,\n",
    "        normalised by MAX_FIELD_DIST (~110.4 m). Negative when the ball moved\n",
    "        away from goal, positive when it moved toward goal.\n",
    "\n",
    "    advancement_factor : float [0, 1]\n",
    "        ADVANCEMENT_WEIGHT × (x_end projected onto the attack axis, normalised\n",
    "        by FIELD_HALF). 0 = ball at or behind the halfway line, 1 = ball at\n",
    "        the opponent's goal line. Linear reward for finishing deep in the\n",
    "        attacking half.\n",
    "\n",
    "    sigmoid_t : float [0, 1]\n",
    "        Logistic sigmoid of (t - 7.5), where t is the play duration in seconds.\n",
    "        Starts near 0 at t = 0 and approaches 1 at t = 15 s, rewarding\n",
    "        sustained possession.\n",
    "\n",
    "    end_bonus : float\n",
    "        SHOT resulting in GOAL  : +1.0\n",
    "        SHOT (no goal)          : +0.3\n",
    "        Ball out of bounds      : -0.1   (last result contains 'OUT')\n",
    "        Interception            : -0.2   (last event_type == 'INTERCEPTION')\n",
    "        Otherwise               :  0.0\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    play_clean : pd.DataFrame\n",
    "        Possession rows with NO_VIDEO removed, sorted chronologically.\n",
    "    attacking_goal_x : float\n",
    "        x-coordinate of the opponent's goal (+52.5 or -52.5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float : P\n",
    "    \"\"\"\n",
    "    if len(play_clean) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    first = play_clean.iloc[0]\n",
    "    last  = play_clean.iloc[-1]\n",
    "\n",
    "    x_start = float(first[\"coordinates_x\"])\n",
    "    y_start = float(first[\"coordinates_y\"])\n",
    "    x_end   = float(last[\"end_coordinates_x\"])\n",
    "    y_end   = float(last[\"end_coordinates_y\"])\n",
    "    goal_y  = 0.0\n",
    "\n",
    "    # --- 1. Ball displacement toward goal [-1, 1] ---\n",
    "    d_start = ((x_start - attacking_goal_x) ** 2 + (y_start - goal_y) ** 2) ** 0.5\n",
    "    d_end   = ((x_end   - attacking_goal_x) ** 2 + (y_end   - goal_y) ** 2) ** 0.5\n",
    "    displacement_scaled = min(1.0, (d_start - d_end) / MAX_FIELD_DIST)\n",
    "\n",
    "    # --- 2. Linear distance from centre-field in the attacking direction [0, 1] ---\n",
    "    # 0 = ball at or behind the halfway line, 1 = ball at the opponent's goal line\n",
    "    attack_sign          = 1.0 if attacking_goal_x > 0 else -1.0\n",
    "    position_from_center = max(0.0, min(1.0, x_end * attack_sign / FIELD_HALF))\n",
    "    advancement_factor   = ADVANCEMENT_WEIGHT * position_from_center\n",
    "\n",
    "    # --- 3. Sigmoid of play duration: ≈ 0 at t=0, ≈ 1 at t=15 s ---\n",
    "    t     = float(last[\"timestamp_sec\"] - first[\"timestamp_sec\"])\n",
    "    sig_t = 1.0 / (1.0 + math.exp(-(t - SIGMOID_SHIFT)))\n",
    "\n",
    "    mult = displacement_scaled * advancement_factor * sig_t\n",
    "\n",
    "    # --- 4. End-of-play event bonus ---\n",
    "    last_type   = str(last[\"event_type\"])\n",
    "    last_result = str(last[\"result\"]).upper()\n",
    "\n",
    "    if last_type == \"SHOT\":\n",
    "        end_bonus = GOAL_BONUS if \"GOAL\" in last_result else SHOT_BONUS\n",
    "    elif \"OUT\" in last_result:\n",
    "        end_bonus = OUT_OF_BOUNDS_PEN\n",
    "    elif last_type == \"INTERCEPTION\":\n",
    "        end_bonus = INTERCEPT_PEN\n",
    "    else:\n",
    "        end_bonus = 0.0\n",
    "\n",
    "    return mult + end_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_VIDEO_THRESHOLD = 3.0  # seconds — discard a play if NO_VIDEO exceeds this\n",
    "N_SYNTHETIC_EDGES  = 2    # how many synthetic long-range hops to add per node\n",
    "                          # k=2 → i→i+2, k=3 → i→i+3, etc.\n",
    "\n",
    "\n",
    "def build_graph(play: pd.DataFrame, home_team_id: str) -> Data | None:\n",
    "    \"\"\"\n",
    "    Build a PyTorch Geometric Data object from one possession sequence.\n",
    "\n",
    "    NO_VIDEO handling:\n",
    "      - Duration of each event = gap to the next event's timestamp.\n",
    "      - If the total NO_VIDEO duration in the play exceeds NO_VIDEO_THRESHOLD,\n",
    "        the entire play is discarded (return None).\n",
    "      - Otherwise, NO_VIDEO rows are removed before building the graph.\n",
    "\n",
    "    Node features (6):\n",
    "      [player_id_enc, coord_x, coord_y, event_type_enc, timestamp_rel, is_under_pressure]\n",
    "\n",
    "    Edge attributes (7):\n",
    "      [edge_type, pass_type_enc, end_x, end_y, action_distance, result_enc, success]\n",
    "      edge_type       : 1.0 = real sequential edge, 2.0 = synthetic long-range edge\n",
    "      action_distance : for real edges — Euclidean distance (metres);\n",
    "                        for synthetic edges — hop proximity weight = 1/k\n",
    "                        (skip-1 k=2 → 0.5, skip-2 k=3 → 0.33, …)\n",
    "\n",
    "    g.y          : tensor([P]) — play score from score_play()\n",
    "    g.player_ids : list[str]   — player_id for each node (parallel to node rows)\n",
    "    g.team_id    : str         — possession team\n",
    "    g.period_id  : int         — period the play occurs in\n",
    "    g.t_start    : float       — timestamp_sec of the first event in the play\n",
    "    \"\"\"\n",
    "    # --- NO_VIDEO check ---\n",
    "    timestamps = play[\"timestamp_sec\"].to_numpy()\n",
    "    durations  = np.diff(timestamps)\n",
    "    durations  = np.append(durations, 0.0)\n",
    "\n",
    "    no_video_mask = (play[\"event_type\"] == \"NO_VIDEO\").to_numpy()\n",
    "    if no_video_mask.any():\n",
    "        if durations[no_video_mask].sum() > NO_VIDEO_THRESHOLD:\n",
    "            return None  # too much missing video — discard play\n",
    "\n",
    "    # Remove NO_VIDEO rows; the remaining events form the graph\n",
    "    play_clean = play[~no_video_mask].reset_index(drop=True)\n",
    "\n",
    "    n = len(play_clean)\n",
    "    if n < 2:\n",
    "        return None\n",
    "\n",
    "    play_start_sec = play_clean[\"timestamp_sec\"].iloc[0]\n",
    "\n",
    "    # --- Node features (N, 6) ---\n",
    "    node_features = [\n",
    "        [\n",
    "            float(row[\"player_id_enc\"]),\n",
    "            float(row[\"coordinates_x\"]),\n",
    "            float(row[\"coordinates_y\"]),\n",
    "            float(row[\"event_type_enc\"]),\n",
    "            float(row[\"timestamp_sec\"] - play_start_sec),\n",
    "            float(row[\"is_under_pressure\"]),\n",
    "        ]\n",
    "        for _, row in play_clean.iterrows()\n",
    "    ]\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    # --- Edges ---\n",
    "    src, dst, attrs = [], [], []\n",
    "    REAL_TYPE  = 1.0\n",
    "    SYNTH_TYPE = 2.0\n",
    "\n",
    "    for i in range(n):\n",
    "        row = play_clean.iloc[i]\n",
    "        x1, y1 = row[\"coordinates_x\"],    row[\"coordinates_y\"]\n",
    "        x2, y2 = row[\"end_coordinates_x\"], row[\"end_coordinates_y\"]\n",
    "        dist    = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "\n",
    "        real_attr = [\n",
    "            REAL_TYPE,\n",
    "            float(row[\"pass_type_enc\"]),\n",
    "            float(x2), float(y2),\n",
    "            float(dist),\n",
    "            float(row[\"result_enc\"]),\n",
    "            float(row[\"success\"]),\n",
    "        ]\n",
    "\n",
    "        if i + 1 < n:  # real sequential edge\n",
    "            src.append(i); dst.append(i + 1); attrs.append(real_attr)\n",
    "        for k in range(2, 2 + N_SYNTHETIC_EDGES):  # synthetic long-range hops\n",
    "            if i + k < n:\n",
    "                hop_weight = 1.0 / k  # 0.5 for skip-1, 0.33 for skip-2, …\n",
    "                synth_attr = [SYNTH_TYPE, 0.0, 0.0, 0.0, hop_weight, 0.0, 0.0]\n",
    "                src.append(i); dst.append(i + k); attrs.append(synth_attr)\n",
    "\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)  # (2, E)\n",
    "    edge_attr  = torch.tensor(attrs, dtype=torch.float)       # (E, 7)\n",
    "\n",
    "    # --- Play score (training label) ---\n",
    "    goal_x = get_attacking_goal_x(\n",
    "        play_clean.iloc[0][\"team_id\"],\n",
    "        play_clean.iloc[0][\"period_id\"],\n",
    "        home_team_id,\n",
    "    )\n",
    "    y = torch.tensor([score_play(play_clean, goal_x)], dtype=torch.float)\n",
    "\n",
    "    # --- Assemble Data object with metadata for lineup / interaction matrix ---\n",
    "    g = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    g.player_ids = play_clean[\"player_id\"].tolist()          # list[str], len == n nodes\n",
    "    g.team_id    = str(play_clean.iloc[0][\"team_id\"])\n",
    "    g.period_id  = int(play_clean.iloc[0][\"period_id\"])\n",
    "    g.t_start    = float(play_clean.iloc[0][\"timestamp_sec\"])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-build-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team_id = str(dataset.metadata.teams[0].team_id)\n",
    "\n",
    "graphs  = []\n",
    "skipped = 0\n",
    "\n",
    "for play in possessions:\n",
    "    g = build_graph(play, home_team_id)\n",
    "    if g is None:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        graphs.append(g)\n",
    "\n",
    "scores = [g.y.item() for g in graphs]\n",
    "print(f\"Graphs built:  {len(graphs)}\")\n",
    "print(f\"Skipped:       {skipped}  (single-event plays or NO_VIDEO > {NO_VIDEO_THRESHOLD}s)\")\n",
    "print(f\"Score — min: {min(scores):.3f}, max: {max(scores):.3f}, mean: {np.mean(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graphs(graphs: list[Data]) -> None:\n",
    "    sizes = [g.num_nodes for g in graphs]\n",
    "    edges = [g.num_edges for g in graphs]\n",
    "\n",
    "    print(f\"Total graphs: {len(graphs)}\")\n",
    "    print(f\"Nodes — min: {min(sizes)}, max: {max(sizes)}, mean: {np.mean(sizes):.1f}\")\n",
    "    print(f\"Edges — min: {min(edges)}, max: {max(edges)}, mean: {np.mean(edges):.1f}\")\n",
    "\n",
    "    for i, g in enumerate(graphs):\n",
    "        assert g.x.shape[1] == 6,          f\"Graph {i}: node feature width {g.x.shape[1]} != 6\"\n",
    "        assert g.edge_attr.shape[1] == 7,  f\"Graph {i}: edge attr width {g.edge_attr.shape[1]} != 7\"\n",
    "        assert g.edge_index.shape[0] == 2, f\"Graph {i}: edge_index shape {g.edge_index.shape}\"\n",
    "        assert g.edge_index.max().item() < g.num_nodes, f\"Graph {i}: out-of-range node index\"\n",
    "        assert g.y.shape == (1,),          f\"Graph {i}: y shape {g.y.shape} != (1,)\"\n",
    "\n",
    "        n = g.num_nodes\n",
    "        expected = max(0, n - 1) + sum(max(0, n - k) for k in range(2, 2 + N_SYNTHETIC_EDGES))\n",
    "        assert g.num_edges == expected, (\n",
    "            f\"Graph {i}: expected {expected} edges for n={n}, N_SYNTHETIC_EDGES={N_SYNTHETIC_EDGES}, got {g.num_edges}\"\n",
    "        )\n",
    "\n",
    "    print(\"All assertions passed.\")\n",
    "\n",
    "    print(\"\\nSample graphs:\")\n",
    "    for g in graphs[:5]:\n",
    "        real  = (g.edge_attr[:, 0] == 1.0).sum().item()\n",
    "        synth = (g.edge_attr[:, 0] == 2.0).sum().item()\n",
    "        print(f\"  nodes={g.num_nodes:>3}, edges={g.num_edges:>3} \"\n",
    "              f\"(real={real:>2}, synthetic={synth:>2})  score={g.y.item():+.4f}\")\n",
    "\n",
    "\n",
    "validate_graphs(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6gh2imp84ny",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer Graph Attention Network that predicts a scalar play score.\n",
    "    Uses edge features (edge_dim=7) in both attention layers.\n",
    "\n",
    "    Architecture\n",
    "    ------------\n",
    "    GATConv(6 → 32, heads=4, concat=True,  edge_dim=7)  →  128-dim per node\n",
    "    BatchNorm1d(128) + ELU + Dropout\n",
    "    GATConv(128 → 32, heads=1, concat=False, edge_dim=7) →  32-dim per node\n",
    "    ELU\n",
    "    global_mean_pool                                      →  32-dim per graph\n",
    "    Linear(32, 1)                                         →  scalar play score\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    add_self_loops=False is required because the graphs were built without\n",
    "    self-loops; PyG's default (True) would try to merge edge_attr with\n",
    "    auto-added self-loop rows, causing a shape mismatch.\n",
    "    \"\"\"\n",
    "\n",
    "    NODE_DIM = 6   # must match g.x.shape[1]\n",
    "    EDGE_DIM = 7   # must match g.edge_attr.shape[1]\n",
    "\n",
    "    def __init__(self, hidden: int = 32, heads: int = 4, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            self.NODE_DIM, hidden, heads=heads, concat=True,\n",
    "            edge_dim=self.EDGE_DIM, dropout=dropout, add_self_loops=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(hidden * heads)  # 128\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            hidden * heads, hidden, heads=1, concat=False,\n",
    "            edge_dim=self.EDGE_DIM, dropout=dropout, add_self_loops=False,\n",
    "        )\n",
    "\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.elu(self.bn1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv2(x, edge_index, edge_attr=edge_attr))\n",
    "        x = global_mean_pool(x, batch)          # (batch_size, 32)\n",
    "        return self.head(x).squeeze(-1)          # (batch_size,)\n",
    "\n",
    "    def forward_with_attention(self, data):\n",
    "        \"\"\"\n",
    "        Single-graph forward pass returning per-layer attention weights.\n",
    "        Useful for post-hoc edge attribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred  : Tensor (1,)\n",
    "        attn1 : Tensor (E, heads)  — Layer 1 per-head attention weights\n",
    "        attn2 : Tensor (E, 1)       — Layer 2 attention weights\n",
    "        \"\"\"\n",
    "        x, ei, ea = data.x, data.edge_index, data.edge_attr\n",
    "        batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        x, (_, attn1) = self.conv1(x, ei, edge_attr=ea, return_attention_weights=True)\n",
    "        x = F.elu(self.bn1(x))\n",
    "        x, (_, attn2) = self.conv2(x, ei, edge_attr=ea, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.head(x).squeeze(-1), attn1, attn2\n",
    "\n",
    "\n",
    "# --- Instantiate and sanity-check ---\n",
    "model = PlayGAT(hidden=32, heads=4, dropout=0.3)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print(f\"\\nTrainable parameters: {total_params:,}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    g0 = graphs[0]\n",
    "    dummy_batch = torch.zeros(g0.num_nodes, dtype=torch.long)\n",
    "    out = model(g0.x, g0.edge_index, g0.edge_attr, dummy_batch)\n",
    "    print(f\"\\nForward check  — nodes: {g0.num_nodes}, output: {out.shape}, value: {out.item():.4f}\")\n",
    "    pred, a1, a2 = model.forward_with_attention(g0)\n",
    "    print(f\"Attention check — attn1: {a1.shape}, attn2: {a2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rn4lywo2vt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Training configuration ───────────────────────────────────────────────────\n",
    "SEED         = 42\n",
    "BATCH_SIZE   = 32\n",
    "LR           = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_EPOCHS     = 50\n",
    "LOG_EVERY    = 10\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ── 80 / 20 reproducible split ────────────────────────────────────────────────\n",
    "perm    = torch.randperm(len(graphs), generator=torch.Generator().manual_seed(SEED)).tolist()\n",
    "n_train = int(0.8 * len(graphs))\n",
    "\n",
    "train_graphs = [graphs[i] for i in perm[:n_train]]\n",
    "val_graphs   = [graphs[i] for i in perm[n_train:]]\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_graphs,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_graphs)} graphs  |  Val: {len(val_graphs)} graphs\")\n",
    "\n",
    "# ── Model, optimiser, loss ────────────────────────────────────────────────────\n",
    "model     = PlayGAT(hidden=32, heads=4, dropout=0.3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses: list[float] = []\n",
    "val_losses:   list[float] = []\n",
    "\n",
    "# ── Training loop ─────────────────────────────────────────────────────────────\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        loss = criterion(pred, batch.y.squeeze(-1))   # batch.y is (B,1); pred is (B,)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch.num_graphs\n",
    "    avg_train = epoch_loss / len(train_graphs)\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pred     = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            val_loss += criterion(pred, batch.y.squeeze(-1)).item() * batch.num_graphs\n",
    "    avg_val = val_loss / len(val_graphs)\n",
    "    val_losses.append(avg_val)\n",
    "\n",
    "    if epoch % LOG_EVERY == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:>3}/{N_EPOCHS}  |  train MSE: {avg_train:.4f}  |  val MSE: {avg_val:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "# ── Loss curve ────────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, N_EPOCHS + 1), train_losses, label=\"Train MSE\")\n",
    "ax.plot(range(1, N_EPOCHS + 1), val_losses,   label=\"Val MSE\",   linestyle=\"--\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss\")\n",
    "ax.set_title(\"PlayGAT Training Curve\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l99sqhjhjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ── Player name lookup ────────────────────────────────────────────────────────\n",
    "player_id_to_name: dict[str, str] = {}\n",
    "for _team in dataset.metadata.teams:\n",
    "    for _p in _team.players:\n",
    "        player_id_to_name[str(_p.player_id)] = _p.name\n",
    "\n",
    "# ── Starting lineups from Kloppy metadata ────────────────────────────────────\n",
    "# player.starting is confirmed True/False in IMPECT open data\n",
    "_team_starters: dict[str, frozenset[str]] = {}\n",
    "for _team in dataset.metadata.teams:\n",
    "    tid      = str(_team.team_id)\n",
    "    starters = frozenset(str(_p.player_id) for _p in _team.players if _p.starting)\n",
    "    _team_starters[tid] = starters\n",
    "    names = [player_id_to_name.get(pid, pid) for pid in sorted(starters)]\n",
    "    print(f\"Team {tid} starters ({len(starters)}): {names}\")\n",
    "\n",
    "# ── Parse SUBSTITUTION events ─────────────────────────────────────────────────\n",
    "_sub_events = df[df[\"event_type\"] == \"SUBSTITUTION\"].copy()\n",
    "print(f\"\\nSubstitution events: {len(_sub_events)}\")\n",
    "\n",
    "_has_receiver = (\n",
    "    \"receiver_player_id\" in df.columns\n",
    "    and _sub_events[\"receiver_player_id\"].notna().any()\n",
    ")\n",
    "print(f\"receiver_player_id populated for subs: {_has_receiver}\")\n",
    "\n",
    "# sub_log[team_id] = sorted list of (period_id, timestamp_sec, off_id, on_id)\n",
    "_sub_log: dict[str, list[tuple[int, float, str, str]]] = defaultdict(list)\n",
    "\n",
    "if _has_receiver and len(_sub_events) > 0:\n",
    "    for _, row in _sub_events.iterrows():\n",
    "        tid    = str(row[\"team_id\"])\n",
    "        period = int(row[\"period_id\"])\n",
    "        t_sec  = float(row[\"timestamp_sec\"])\n",
    "        off_id = str(row[\"player_id\"])\n",
    "        on_id  = str(row.get(\"receiver_player_id\", float(\"nan\")))\n",
    "        if on_id not in (\"nan\", \"UNKNOWN\", \"None\"):\n",
    "            _sub_log[tid].append((period, t_sec, off_id, on_id))\n",
    "    for tid, subs in _sub_log.items():\n",
    "        subs.sort(key=lambda x: (x[0], x[1]))\n",
    "        print(f\"  Team {tid}: {len(subs)} substitution(s) tracked\")\n",
    "else:\n",
    "    print(\"  Falling back to period-appearance heuristic.\")\n",
    "\n",
    "_sub_log_frozen       = dict(_sub_log)\n",
    "_team_starters_frozen = dict(_team_starters)\n",
    "\n",
    "\n",
    "def get_lineup_at_time(team_id: str, period_id: int, timestamp_sec: float) -> frozenset[str]:\n",
    "    \"\"\"\n",
    "    Return the set of player IDs on the field for `team_id` at the given time.\n",
    "\n",
    "    Primary path (when substitution data is available):\n",
    "      Starts from the team's starting XI, applies any subs that occurred at\n",
    "      or before (period_id, timestamp_sec) in chronological order.\n",
    "\n",
    "    Fallback path (no sub data):\n",
    "      Returns all players who appeared in any event for this team in this\n",
    "      period up to this timestamp (safe over-approximation).\n",
    "\n",
    "    Always excludes \"UNKNOWN\".\n",
    "    \"\"\"\n",
    "    tid = str(team_id)\n",
    "\n",
    "    if _sub_log_frozen:\n",
    "        lineup = set(_team_starters_frozen.get(tid, frozenset()))\n",
    "        for s_period, s_t, off_id, on_id in _sub_log_frozen.get(tid, []):\n",
    "            if (s_period, s_t) <= (period_id, timestamp_sec):\n",
    "                lineup.discard(off_id)\n",
    "                lineup.add(on_id)\n",
    "        lineup.discard(\"UNKNOWN\")\n",
    "        return frozenset(lineup)\n",
    "    else:\n",
    "        mask = (\n",
    "            (df[\"team_id\"].astype(str) == tid) &\n",
    "            (\n",
    "                (df[\"period_id\"] < period_id) |\n",
    "                ((df[\"period_id\"] == period_id) & (df[\"timestamp_sec\"] <= timestamp_sec))\n",
    "            ) &\n",
    "            (df[\"player_id\"] != \"UNKNOWN\")\n",
    "        )\n",
    "        players = set(df[mask][\"player_id\"].unique())\n",
    "        players.discard(\"UNKNOWN\")\n",
    "        return frozenset(players)\n",
    "\n",
    "\n",
    "# ── Sanity check ──────────────────────────────────────────────────────────────\n",
    "for _team in dataset.metadata.teams:\n",
    "    tid = str(_team.team_id)\n",
    "    lu  = get_lineup_at_time(tid, 1, 0.0)\n",
    "    print(f\"\\nTeam {tid} at kick-off ({len(lu)} players):\")\n",
    "    for pid in sorted(lu):\n",
    "        print(f\"  {pid}: {player_id_to_name.get(pid, '???')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2ao4nzx22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 1: Model inference on every graph ───────────────────────────────────\n",
    "# Iterate one graph at a time (DataLoader not used here because g.player_ids /\n",
    "# g.team_id / g.period_id / g.t_start are non-tensor attributes that are not\n",
    "# collated by PyG's Batch).\n",
    "model.eval()\n",
    "predicted_scores: list[float] = []\n",
    "with torch.no_grad():\n",
    "    for g in graphs:\n",
    "        dummy_batch = torch.zeros(g.num_nodes, dtype=torch.long)\n",
    "        pred = model(g.x, g.edge_index, g.edge_attr, dummy_batch)\n",
    "        predicted_scores.append(pred.item())\n",
    "\n",
    "print(f\"Inference complete: {len(predicted_scores)} graphs\")\n",
    "print(f\"Predicted score range: [{min(predicted_scores):.3f}, {max(predicted_scores):.3f}]\")\n",
    "\n",
    "# ── Step 2: Build player universe (both teams, excluding UNKNOWN) ─────────────\n",
    "all_pids: list[str] = sorted(\n",
    "    {pid for g in graphs for pid in g.player_ids if pid != \"UNKNOWN\"}\n",
    ")\n",
    "n_players  = len(all_pids)\n",
    "pid_to_idx = {pid: i for i, pid in enumerate(all_pids)}\n",
    "away_team_id = str(dataset.metadata.teams[1].team_id)\n",
    "print(f\"Unique players in graphs: {n_players}\")\n",
    "\n",
    "# ── Step 3: Accumulate statistics ─────────────────────────────────────────────\n",
    "# score_sum[i, j]          — weighted sum of predicted scores over plays where i & j interact\n",
    "# participation_count[i]   — # plays player i appeared in as a node\n",
    "# field_plays[i]           — # plays during which player i was on the field\n",
    "# field_pairs[i, j]        — # plays during which both i and j were on field\n",
    "\n",
    "score_sum           = np.zeros((n_players, n_players), dtype=np.float64)\n",
    "participation_count = np.zeros(n_players,               dtype=np.float64)\n",
    "field_plays         = np.zeros(n_players,               dtype=np.float64)\n",
    "field_pairs         = np.zeros((n_players, n_players),  dtype=np.float64)\n",
    "\n",
    "for g, pred_score in zip(graphs, predicted_scores):\n",
    "    # ── Full on-field lineup for this play (both teams) ───────────────────\n",
    "    opp_tid     = away_team_id if g.team_id == home_team_id else home_team_id\n",
    "    lineup_poss = get_lineup_at_time(g.team_id, g.period_id, g.t_start)\n",
    "    lineup_opp  = get_lineup_at_time(opp_tid,   g.period_id, g.t_start)\n",
    "    full_lineup = lineup_poss | lineup_opp\n",
    "    lu_indices  = [pid_to_idx[pid] for pid in full_lineup if pid in pid_to_idx]\n",
    "\n",
    "    # field_plays denominator\n",
    "    for li in lu_indices:\n",
    "        field_plays[li] += 1.0\n",
    "\n",
    "    # field_pairs denominator (symmetric)\n",
    "    for ii, li in enumerate(lu_indices):\n",
    "        for lj in lu_indices[ii:]:\n",
    "            field_pairs[li, lj] += 1.0\n",
    "            if li != lj:\n",
    "                field_pairs[lj, li] += 1.0\n",
    "\n",
    "    # ── Diagonal: each unique participating player ─────────────────────────\n",
    "    participating = {pid_to_idx[pid] for pid in g.player_ids if pid in pid_to_idx}\n",
    "    for pi in participating:\n",
    "        score_sum[pi, pi]       += pred_score\n",
    "        participation_count[pi] += 1.0\n",
    "\n",
    "    # ── Off-diagonal: edge-linked player PAIRS (hop-proximity weighted) ────\n",
    "    # Real edges (edge_type=1.0) get weight 1.0.\n",
    "    # Synthetic skip-k edges store hop proximity weight 1/k in action_distance (index 4).\n",
    "    # If a pair is connected by multiple edges, keep the highest weight (max → real wins).\n",
    "    pair_weights: dict[tuple[int, int], float] = {}\n",
    "    for e in range(g.edge_index.shape[1]):\n",
    "        src_pid = g.player_ids[g.edge_index[0, e].item()]\n",
    "        dst_pid = g.player_ids[g.edge_index[1, e].item()]\n",
    "        if src_pid == \"UNKNOWN\" or dst_pid == \"UNKNOWN\":\n",
    "            continue\n",
    "        if src_pid not in pid_to_idx or dst_pid not in pid_to_idx:\n",
    "            continue\n",
    "        pi, pj = pid_to_idx[src_pid], pid_to_idx[dst_pid]\n",
    "        if pi == pj:\n",
    "            continue\n",
    "        pair   = (min(pi, pj), max(pi, pj))\n",
    "        weight = 1.0 if g.edge_attr[e, 0].item() == 1.0 else g.edge_attr[e, 4].item()\n",
    "        pair_weights[pair] = max(pair_weights.get(pair, 0.0), weight)\n",
    "\n",
    "    for (pi, pj), w in pair_weights.items():\n",
    "        score_sum[pi, pj] += pred_score * w\n",
    "        score_sum[pj, pi] += pred_score * w\n",
    "\n",
    "# ── Step 4: Build the interaction matrix ──────────────────────────────────────\n",
    "#\n",
    "# Off-diagonal [i,j]:\n",
    "#   = weighted_score_sum[i,j] / field_pairs[i,j]\n",
    "#   Weighting by hop proximity dampens inflation from distant synthetic connections.\n",
    "#\n",
    "# Diagonal [i,i]:\n",
    "#   = avg_predicted_score_when_player_participates  ×  P(participates | on field)\n",
    "#   = score_sum[i,i] / field_plays[i]\n",
    "\n",
    "interaction_matrix = np.zeros((n_players, n_players), dtype=np.float64)\n",
    "\n",
    "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "    off = np.where(field_pairs > 0, score_sum / field_pairs, 0.0)\n",
    "    np.fill_diagonal(off, 0.0)\n",
    "    interaction_matrix += off\n",
    "\n",
    "    diag_vals = np.where(field_plays > 0, np.diag(score_sum) / field_plays, 0.0)\n",
    "    np.fill_diagonal(interaction_matrix, diag_vals)\n",
    "\n",
    "# ── Step 5a: DataFrame display ────────────────────────────────────────────────\n",
    "player_names   = [player_id_to_name.get(pid, pid) for pid in all_pids]\n",
    "interaction_df = pd.DataFrame(interaction_matrix, index=player_names, columns=player_names)\n",
    "\n",
    "print(\"\\nPlayer Interaction Matrix (top-left 8 × 8):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.3f}\".format, \"display.max_columns\", 8):\n",
    "    print(interaction_df.iloc[:8, :8])\n",
    "\n",
    "# ── Step 5b: Heatmap ──────────────────────────────────────────────────────────\n",
    "fig_size = max(10, n_players * 0.55)\n",
    "fig, ax  = plt.subplots(figsize=(fig_size, fig_size * 0.85))\n",
    "\n",
    "vmax = np.abs(interaction_matrix).max() or 1.0\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "im   = ax.imshow(interaction_matrix, cmap=\"RdYlGn\", norm=norm, aspect=\"auto\")\n",
    "\n",
    "ax.set_xticks(range(n_players))\n",
    "ax.set_yticks(range(n_players))\n",
    "ax.set_xticklabels(player_names, rotation=90, fontsize=7)\n",
    "ax.set_yticklabels(player_names, fontsize=7)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=\"hop-weighted avg predicted score / field-time probability\")\n",
    "ax.set_title(\n",
    "    f\"Player Interaction Matrix — Match {MATCH_ID}\\n\"\n",
    "    \"Diagonal: avg score × P(participate | on field)   \"\n",
    "    \"Off-diagonal: hop-weighted avg score × P(linked by edge | both on field)\",\n",
    "    fontsize=9,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Value range:  [{interaction_matrix.min():.4f}, {interaction_matrix.max():.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (Soccer Hackathon)",
   "language": "python",
   "name": "soccer-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
